---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a first-year Ph.D. student at the ROAS Thrust at the Hong Kong University of Science and Technology (HKUST), Guangzhou campus, advised by [Haoang Li](https://sites.google.com/view/haoangli/homepage). I received my Bachelor's degree in robotics at Harbin Institute of Technology (HIT), Weihai Campus, advised by Minghang Zhao and Bo Huang. I was a visiting student at MiLab, Westlake University, supervised by Donglin Wang. I keep a close connection with [Pengxiang Ding](https://dingpx.github.io/) and [Han Zhao](https://h-zhao1997.github.io/) from then on, who have been teaching me a lot. I also spent time at Monash University, supervised by Zongyuan Ge and Xuelian Cheng.

Goal: Pushing the boundaries of the world through robotics.

Focus: Building dexterous and generalizable robotic systems through **Vision-Language-Action** models and **world models**.

Email: songwenxuan0115 [AT] gmail.com

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# üî• Projects
- [Embodied-AI-Paper-TopConf](https://github.com/Songwxuan/Embodied-AI-Paper-TopConf) ![GitHub stars](https://img.shields.io/github/stars/Songwxuan/Embodied-AI-Paper-TopConf?style=social): We collect published papers in the field of embodied intelligence.

- [LLaVA-VLA](https://github.com/OpenHelix-Team/LLaVA-VLA) ![GitHub stars](https://img.shields.io/github/stars/OpenHelix-Team/LLaVA-VLA?style=social): We propose a simple yet effective Vision-Language-Action model built upon the popular open-source VLM LLaVA.

- [OpenHelix](https://github.com/OpenHelix-robot/OpenHelix) ![GitHub stars](https://img.shields.io/github/stars/OpenHelix-robot/OpenHelix?style=social): We propose an open-source dual-system VLA model for robotic manipulation.

# üìù Selected Publications 

- For full publications, please refer to my Google Scholar.

<div class='paper-box'><div class='paper-box-image'><div><img src='images/ceed-vla.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 

[CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding](https://arxiv.org/pdf/2506.13725)

**Wenxuan Song¬π\***, **Jiayi Chen¬π\***, **Pengxiang Ding¬≤Àí¬≥‚Ä†**, **Yuxin Huang¬π**, **Han Zhao¬≤Àí¬≥**, **Donglin Wang¬≤**,  **Haoang Li¬π‚Ä°**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IROS2025</div><img src='images/PD-VLA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](https://arxiv.org/pdf/2503.02310)

**Wenxuan Song¬π\***, **Jiayi Chen¬π\***, **Pengxiang Ding¬≤Àí¬≥**, **Han Zhao¬≤Àí¬≥**, **Wei Zhao¬≤**,**Zhide Zhong¬π**,  **Zongyuan Ge‚Å¥**, **Jun Ma¬π**,  **Haoang Li¬π‚Ä°**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/openhelix.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 

[OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation](https://openhelix-robot.github.io/)

**Can Cui**, **Pengxiang Ding***, **Wenxuan Song**, **Shuanghao Bai**, **Xinyang Tong**, **Zirui Ge**, **Runze Suo**,  
**Wanqi Zhou**, **Yang Liu**, **Bofang Jia**, **Han Zhao**, **Siteng Huang**, **Donglin Wang‚Ä†**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/rational-vla.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RationalVLA: A Rational Vision-Language-Action Model with Dual System](https://arxiv.org/pdf/2506.10826)

**Wenxuan Song, Jiayi Chen, Wenxue Li, Xu He, Han Zhao, Can Cui, Pengxiang Ding, Shiyan Su, Feilong Tang, Donglin Wang, Xuelian Cheng, Zongyuan Ge, Xinhu Zheng, Zhe Liu, Hesheng Wang, Haoang Li**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/more.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 

[MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models](https://arxiv.org/abs/2503.08007)

**Han Zhao\***, **Wenxuan Song\***, **Donglin Wang**, **Xinyang Tong**, **Pengxiang Ding**, **Xuelian Cheng**, **Zongyuan Ge**

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/germ.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 

[GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robots](https://songwxuan.github.io/GeRM/)

**Wenxuan Song**, **Han Zhao**, **Pengxiang Ding**, **Can Cui**, **Shangke Lyu**, **Yaning Fan**, **Donglin Wang**  

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><img src='images/quart.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1"> 
  
[QUAR-VLA: Vision-Language-Action Model for Quadruped Robots](https://quart-robot.github.io/)

**Pengxiang Ding**, **Han Zhao**, **Wenxuan Song**, **Wenjie Zhang**, **Min Zhang**, **Siteng Huang**, **Ningxi Yang**, **Donglin Wang**

</div>
</div>
  
<!-- - [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020** -->

<!-- # üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

<!-- # üìñ Educations
- *2023.06 - 2025.06 (now)*, Pursuing a Master's degree in Computer Science and Technology at Harbin Institute of Technology.
- *2019.09 - 2023.06*, Studied Automation at Tongji University and obtained a Bachelor's degree in Engineering. -->

<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

